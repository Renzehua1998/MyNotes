# 复杂网络夏令营2-关系型数据挖掘

关系型数据挖掘（Relational data mining）

- 中心性度量
- 图模型
- 基准（benchmarks）

> 简介：使用jupyter notebook、python3、igraph包

## 关系型数据

### 术语

1. 监督学习：分类（classification）、回归（regression）
2. 无监督学习：聚类（clustering）、密度估计（density estimation）、降维（dimension reduction）、异常值检测（outlier detection）。
3. 半监督学习：标记的数据很少，**同时使用标记和未标记的数据**

- 特征：feature（连续、分类或排序）

### 关系型数据

实体表示为点，关系可以表示为边，或超边（是否加权、是否有向）

- A和B是朋友
- A给B发邮件
- A，B和C在同一个团队

![image-20221125010217181](2关系型数据挖掘/image-20221125010217181.png)

关系型数据被建模为图（graphs）或超图（hypergraphs）

$G = (V , E)$, let $n = |V | $and $m = |E|$——得到邻接矩阵（一些性质，不再赘述）

完全图（Complete graph），也叫集团（clique）

---

图会出现在不同场景中：电子邮件交换、蛋白质-蛋白质相互作用图、社交关系（空手道俱乐部）、赛事（大学足球队之间的比赛）

### Issues

一个图里面有很多社区，传统机器学习将向量空间进行嵌入降维后可以进行聚类。

有许多工具可以处理这些数据，抽样等统计技术可用于处理大型数据集。

采样保留关键属性（簇、平均距离等），但这些都**在图形空间中被破坏**。

### Problems

1. 中心性度量
2. 寻找社区
3. 异常检测
4. 种子集扩展（seed set expansion）-局部采样
5. 链路（边缘）预测
6. 半监督学习
7. 向量空间嵌入

## 中心性

### 度的定义

所有边权值之和

入度、出度：入边权值和、出边权值和

无权边通过除以$n − 1 = |V | − 1$来进行归一化（假设没有自环）

### 节点中心性

- **邻居中心性**

中心性（Centrality）：综合考虑节点度和连接到高度数中心的其他节点
$$
c(i)=\sum_{j \in V} a_{i j} c(j)
$$
$\operatorname{det}(A-I) \neq 0$，唯一解为$c=0$——连通图无定义，需要进行扩充定义

- 定义一：**特征向量中心性**（eigenvector centrality）

将特征向量中心性定义为与（neighbour’s centrality）成比例

节点i的特征向量中心性是以下方程的解：
$$
\lambda c^E(i)=\sum_{j \in V} a_{i j} c^E(j)=A c^E
$$
有向图，入度和出度分别进行定义

对于连通图，我们测量与最大特征值相对应的特征向量的中心性，最大特征值是实的和正的（Perron Frobenius），即：
$$
c^E(i)=u_1(i)
$$
u1是主导特征向量

- 定义二：**接近中心性**（closeness centrality）

考虑节点之间的最短路径（最小跳数或最小边权重之和）——测地线（geodesic）
$$
c^c(i)=\left(\sum_{j \neq i} d_{i j}\right)^{-1}
$$
dij为最短路径长度，不可达定义为无穷，无权图可定义为节点数n

- 定义三：**介数中心性**（betweenness centrality）

更常用的方法是考虑所有测地线，定义$n_{jk}(i)$为节点j和k之间通过节点i的测地线的数量
$$
c^B(i)=\sum_{j \neq i} \sum_{k \neq i, j} \frac{n_{j k}(i)}{n_{j k}}
$$

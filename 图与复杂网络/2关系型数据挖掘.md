# 复杂网络夏令营2-关系型数据挖掘

关系型数据挖掘（Relational data mining）

- 中心性度量
- 图模型
- 基准（benchmarks）

> 简介：使用jupyter notebook、python3、igraph包

## 关系型数据

### 术语

1. 监督学习：分类（classification）、回归（regression）
2. 无监督学习：聚类（clustering）、密度估计（density estimation）、降维（dimension reduction）、异常值检测（outlier detection）。
3. 半监督学习：标记的数据很少，**同时使用标记和未标记的数据**

- 特征：feature（连续、分类或排序）

### 关系型数据

实体表示为点，关系可以表示为边，或超边（是否加权、是否有向）

- A和B是朋友
- A给B发邮件
- A，B和C在同一个团队

![image-20221125010217181](2关系型数据挖掘/image-20221125010217181.png)

关系型数据被建模为图（graphs）或超图（hypergraphs）

$G = (V , E)$, let $n = |V | $and $m = |E|$——得到邻接矩阵（一些性质，不再赘述）

完全图（Complete graph），也叫集团（clique）

---

图会出现在不同场景中：电子邮件交换、蛋白质-蛋白质相互作用图、社交关系（空手道俱乐部）、赛事（大学足球队之间的比赛）

### Issues

一个图里面有很多社区，传统机器学习将向量空间进行嵌入降维后可以进行聚类。

有许多工具可以处理这些数据，抽样等统计技术可用于处理大型数据集。

采样保留关键属性（簇、平均距离等），但这些都**在图形空间中被破坏**。

### Problems

1. 中心性度量
2. 寻找社区
3. 异常检测
4. 种子集扩展（seed set expansion）-局部采样
5. 链路（边缘）预测
6. 半监督学习
7. 向量空间嵌入

## 中心性

### 度中心性

所有边权值之和

入度、出度：入边权值和、出边权值和

归一化度中心性：无权图（出入）度除以$n − 1 = |V | − 1$来获得（假设没有自环）

### 节点中心性

**中心性**（Centrality）：综合考虑节点度和连接到高度数中心的其他节点
$$
c(i)=\sum_{j \in V} a_{i j} c(j)
$$
$\operatorname{det}(A-I) \neq 0$，唯一解为$c=0$——连通图无定义，需要进行扩充定义

- 定义一：**特征向量中心性**（eigenvector centrality）

  将特征向量中心性定义为与（neighbour’s centrality）成比例

  节点i的特征向量中心性是以下方程的解：
  $$
  \lambda c^E(i)=\sum_{j \in V} a_{i j} c^E(j) \\
  \lambda c^E=A c^E
  $$
  有向图，入度和出度分别进行定义

  对于连通图，我们测量与最大特征值相对应的特征向量的中心性，最大特征值是实的和正的（Perron Frobenius），即：
  $$
  c^E(i)=u_1(i)
  $$
  u1是主导特征向量

- 定义二：**接近中心性**（closeness centrality）

  考虑节点之间的最短路径（最小跳数或最小边权重之和）——测地线（geodesic）
  $$
  c^c(i)=\left(\sum_{j \neq i} d_{i j}\right)^{-1}
  $$
  dij为最短路径长度，dii=0，不可达定义为无穷，无权图可定义为节点数n

- 定义三：**介数中心性**（betweenness centrality）

  更常用的方法是考虑所有测地线：

  - 定义$n_{jk}$为节点j和k之间所有的测地线的数量
  - 定义$n_{jk}(i)$为节点j和k之间通过节点i的测地线的数量

  $$
  c^B(i)=\sum_{j \neq i} \sum_{k \neq i, j} \frac{n_{j k}(i)}{n_{j k}}
  $$

- 定义四：**Δ中心性**（delta-centrality）

  考虑从G中删除某些节点i的影响

  - 设P(G)是图G上性能的一些度量
  - 设Gi是通过从图G移除与节点i相邻的所有边而获得的图

  $$
  c^{\Delta}(i)=\frac{P(G)-P\left(G_i\right)}{P(G)}
  $$

  ——注意，我们要保证$P(G)-P(G_i) > 0$

  - 一种可能的P(G)取值：定义图G的影响力E(G)——
    $$
    E(G)=\frac{1}{n(n-1)} \sum_i \sum_{j \neq i} \frac{1}{d_{i j}}
    $$

  说明：

  - 对于不连通图，dij=∞的节点对的贡献为零。
  - 通过考虑节点集，所有中心性度量都可以推广到组中心性度量。——节点集的度定义为非g节点连接到g内节点的数量

- 定义五：谷歌**PageRank**算法中使用的中心性

  思想：互联网中重要页面与许多重要页面相互链接

  - 我们在节点i处随机游走：

    - 以概率$1−d$：跳到随机节点$j∈V$。
    - 以概率$d$：以概率$a_{ij}/d^{out}_i$跳到节点j。
    - 可以用代数或迭代（幂）方法获得PageRank值的解

    $$
    R(i)=\frac{1-d}{n}+d \sum_{j \rightarrow i} \frac{R(j)}{d_j^{\text {out }}}
    $$

  - 在有向图中，我们将中枢（hub）定义为具有高出度的节点，而将权威（authority）定义为具有高度入度的节点。
  - 在无向图中，这两个概念是相同的。

- 定义六：**中枢/权威中心性**（hub/ authority）

  中枢中心性：
  $$
  c^H(i)=\nu \sum_j a_{i j} c^A(j)
  $$
  权威中心性：
  $$
  c^A(i)=\mu \sum_j a_{j i} c^H(j)
  $$
  让$\lambda=\frac{1}{\nu \mu}$，我们有：
  $$
  \begin{aligned}
  & A^t A c^A=\lambda c^A \\
  & A A^t c^H=\lambda c^H
  \end{aligned}
  $$

### 边中心性

- $n_{j k}$：节点j和k之间的测地线数
- $n_{j k}(e)$：通过边e的节点j和k之间的测地线数。

$$
c^B(e)=\sum_j \sum_{k \neq j} \frac{n_{j k}(e)}{n_{j k}}
$$

### 相关性

基础知识：

- 设pi为出现次数为i次的节点在G中的比例，即**度分布**
- 度相关性 度量 由边链接的节点的度之间的关系
- 在同配网络（assortative network）中，高阶节点倾向于更多地连接到其他高阶节点，低阶节点也是如此
- 在异配网络（disassortative network）中，高阶节点倾向于更多地连接到低阶节点，反之亦然。

定义：

- k度节点的**平均最近邻度函数**：
  $$
  k_{n n}(k)=\sum_{k^{\prime}} k^{\prime} P\left(k^{\prime} \mid k\right)
  $$

  - 其中$P\left(k^{\prime} \mid k\right)$是从k度节点出到k'度节点入的概率

  - 如果没有相关性（中性网络-neutral network），有
    $$
    k_{n n}(k)=\frac{\sum_i i^2 \cdot p_i}{\sum_i i \cdot p_i}
    $$
    pi：i度节点的比例

  友谊悖论指出，节点邻居的平均度通常高于其自身的度。这是由于在许多网络中：
  $$
  \frac{\sum_i i^2 \cdot p_i}{\sum_i i \cdot p_i}>>\sum_i i \cdot p_i
  $$
  即：一个节点更可能与中心（高阶节点）链接

- 度相关函数可以近似为：
  $$
  k_{n n}(k)=a k^\mu
  $$

  - $µ>0$表示同配网络
  - $µ≈0$表示中性网络
  - $µ<0$表示异配网络